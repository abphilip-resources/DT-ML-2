{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89f0bbe",
   "metadata": {
    "papermill": {
     "duration": 0.003182,
     "end_time": "2023-03-18T04:20:35.561365",
     "exception": false,
     "start_time": "2023-03-18T04:20:35.558183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**This notebook is an exercise in the [Intro to Game AI and Reinforcement Learning](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/deep-reinforcement-learning).**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e093df",
   "metadata": {
    "papermill": {
     "duration": 0.001913,
     "end_time": "2023-03-18T04:20:35.565733",
     "exception": false,
     "start_time": "2023-03-18T04:20:35.563820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In the tutorial, you learned a bit about reinforcement learning and used the `stable-baselines3` package to train an agent to beat a random opponent.  In this exercise, you will check your understanding and tinker with the code to deepen your intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f041f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T04:20:35.572489Z",
     "iopub.status.busy": "2023-03-18T04:20:35.571627Z",
     "iopub.status.idle": "2023-03-18T04:20:35.605070Z",
     "shell.execute_reply": "2023-03-18T04:20:35.604088Z"
    },
    "papermill": {
     "duration": 0.039525,
     "end_time": "2023-03-18T04:20:35.607388",
     "exception": false,
     "start_time": "2023-03-18T04:20:35.567863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.game_ai.ex4 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d57d8",
   "metadata": {
    "papermill": {
     "duration": 0.002016,
     "end_time": "2023-03-18T04:20:35.611734",
     "exception": false,
     "start_time": "2023-03-18T04:20:35.609718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1) Set the architecture\n",
    "\n",
    "In the tutorial, you learned one way to design a neural network that can select moves in Connect Four.  The neural network had an output layer with seven nodes: one for each column in the game board.\n",
    "\n",
    "Say now you wanted to create a neural network that can play chess.  How many nodes should you put in the output layer?\n",
    "\n",
    "- Option A: 2 nodes (number of game players)\n",
    "- Option B: 16 nodes (number of game pieces that each player starts with)\n",
    "- Option C: 4672 nodes (number of possible moves)\n",
    "- Option D: 64 nodes (number of squares on the game board)\n",
    "\n",
    "Use your answer to set the value of the `best_option` variable below.  Your answer should be one of `'A'`, `'B'`, `'C'`, or `'D'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66208e2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T04:20:35.617889Z",
     "iopub.status.busy": "2023-03-18T04:20:35.617028Z",
     "iopub.status.idle": "2023-03-18T04:20:35.626367Z",
     "shell.execute_reply": "2023-03-18T04:20:35.625364Z"
    },
    "papermill": {
     "duration": 0.014754,
     "end_time": "2023-03-18T04:20:35.628734",
     "exception": false,
     "start_time": "2023-03-18T04:20:35.613980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_PickBestOption\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#33cc33\">Correct:</span> If we use a similar network as in the tutorial, the network should output a probability for each possible move."
      ],
      "text/plain": [
       "Correct: If we use a similar network as in the tutorial, the network should output a probability for each possible move."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fill in the blank\n",
    "best_option = 'C'\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff71dcd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T04:20:35.635395Z",
     "iopub.status.busy": "2023-03-18T04:20:35.634537Z",
     "iopub.status.idle": "2023-03-18T04:20:35.638860Z",
     "shell.execute_reply": "2023-03-18T04:20:35.637981Z"
    },
    "papermill": {
     "duration": 0.009731,
     "end_time": "2023-03-18T04:20:35.640978",
     "exception": false,
     "start_time": "2023-03-18T04:20:35.631247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lines below will give you solution code\n",
    "# q_1.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f73dc",
   "metadata": {
    "papermill": {
     "duration": 0.002269,
     "end_time": "2023-03-18T04:20:35.645665",
     "exception": false,
     "start_time": "2023-03-18T04:20:35.643396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2) Decide reward\n",
    "\n",
    "In the tutorial, you learned how to give your agent a reward that encourages it to win games of Connect Four.  Consider now training an agent to win at the game [Minesweeper](https://bit.ly/2T5xEY8).  The goal of the game is to clear the board without detonating any bombs.\n",
    "\n",
    "To play this game in Google Search, click on the **[Play]** button at [this link](https://www.google.com/search?q=minesweeper).  \n",
    "\n",
    "<center>\n",
    "<img src=\"https://i.imgur.com/WzoEfKY.png\" width=50%><br/>\n",
    "</center>\n",
    "\n",
    "With each move, one of the following is true:\n",
    "- The agent selected an invalid move (in other words, it tried to uncover a square that was uncovered as part of a previous move).  Let's assume this ends the game, and the agent loses.\n",
    "- The agent clears a square that did not contain a hidden mine.  The agent wins the game, because all squares without mines are revealed.\n",
    "- The agent clears a square that did not contain a hidden mine, but has not yet won or lost the game.\n",
    "- The agent detonates a mine and loses the game.\n",
    "\n",
    "How might you specify the reward for each of these four cases, so that by maximizing the cumulative reward, the agent will try to win the game?\n",
    "\n",
    "After you have decided on your answer, run the code cell below to get credit for completing this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af46c15c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T04:20:35.652364Z",
     "iopub.status.busy": "2023-03-18T04:20:35.651514Z",
     "iopub.status.idle": "2023-03-18T04:20:35.659544Z",
     "shell.execute_reply": "2023-03-18T04:20:35.658490Z"
    },
    "papermill": {
     "duration": 0.013388,
     "end_time": "2023-03-18T04:20:35.661561",
     "exception": false,
     "start_time": "2023-03-18T04:20:35.648173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"questionId\": \"2_DecideReward\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#33cc99\">Solution:</span> Here's a possible solution - after each move, we give the agent a reward that tells it how well it did:\n",
       "- If agent wins the game in that move, it gets a reward of `+1`.\n",
       "- Else if the agent selects an invalid move, it gets a reward of `-10`.\n",
       "- Else if it detonates a mine, it gets a reward of `-1`.\n",
       "- Else if the agent clears a square with no hidden mine, it gets a reward of `+1/100`.\n",
       "\n",
       "To check the validity of your answer, note that the reward for selecting an invalid move and for detonating a mine should both be negative.  The reward for winning the game should be positive.  And, the reward for clearing a square with no hidden mine should be either zero or slightly positive."
      ],
      "text/plain": [
       "Solution: Here's a possible solution - after each move, we give the agent a reward that tells it how well it did:\n",
       "- If agent wins the game in that move, it gets a reward of `+1`.\n",
       "- Else if the agent selects an invalid move, it gets a reward of `-10`.\n",
       "- Else if it detonates a mine, it gets a reward of `-1`.\n",
       "- Else if the agent clears a square with no hidden mine, it gets a reward of `+1/100`.\n",
       "\n",
       "To check the validity of your answer, note that the reward for selecting an invalid move and for detonating a mine should both be negative.  The reward for winning the game should be positive.  And, the reward for clearing a square with no hidden mine should be either zero or slightly positive."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check your answer (Run this code cell to receive credit!)\n",
    "q_2.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef2fae",
   "metadata": {
    "papermill": {
     "duration": 0.002523,
     "end_time": "2023-03-18T04:20:35.666824",
     "exception": false,
     "start_time": "2023-03-18T04:20:35.664301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have completed the course, and it's time to put your new skills to work!  \n",
    "\n",
    "The next step is to apply what you've learned to a **[more complex game: Halite](https://www.kaggle.com/c/halite)**.  For a step-by-step tutorial in how to make your first submission to this competition, **[check out the bonus lesson](https://www.kaggle.com/alexisbcook/getting-started-with-halite)**!\n",
    "\n",
    "You can find more games as they're released on the **[Kaggle Simulations page](https://www.kaggle.com/simulations)**.\n",
    "\n",
    "As we did in the course, we recommend that you start simple, with an agent that follows your precise instructions.  This will allow you to learn more about the mechanics of the game and to build intuition for what makes a good agent.  Then, gradually increase the complexity of your agents to climb the leaderboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021459d",
   "metadata": {
    "papermill": {
     "duration": 0.002522,
     "end_time": "2023-03-18T04:20:35.672123",
     "exception": false,
     "start_time": "2023-03-18T04:20:35.669601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Allen**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.2496,
   "end_time": "2023-03-18T04:20:36.195139",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-18T04:20:26.945539",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
